# Crystal's assignment for week 03
## *How do technical tools promise to "fair out" the remaining discrimination that exist in social/welfare systems? In how far can they succeed, in which ways do they fail?*

The remaining discrimination existing in social/welfare systems is mainly on women of colour, sexual minority and other groups of people. They were blocked to the access of economic aid or other benefit. And the universal rules that can apply to everyone, regardless of the race, gender or sexual orientation, could solve these kinds of discrimination to some extent. Since the algorithm or technical systems set up and follow the same rules to every user, people with different race, gender or sociocultural background own the same standards and follow the same rules. Besides, differing from social or caseworkers, the algorithm doesn't have its own preference and aversion, let alone adding subjective emotion to its analysis or collecting data. These factors can somehow guarantee the objectivity and fairness of the social/welfare system. However, according to last week's broadcast, the speaker says that "people who make system shape the way it sees the world". Therefore, the developer's experience about the poverty or welfare has great effect on the efficiency and availability of the system. For example, the organization needs to use survey to investigate unhoused people and then decide who can get the house. The questions and options are the key for the evaluation of eligibility. People need to reach the standard score, which is based on their answers, to get a house. Since the contents are made by the developers and other related people, and these questions are impossible to apply to every unhoused people. Therefore, unfairness happened when people only rely on the result of algorithm and neglect the individual issues and information to decide the eligibility for the house.  

## *Imagine, what could this (following quotes) mean in the widest sense? "The state doesn't need a cop to kill a person" and "electronic incarceration"*

The algorithm of some social/welfare systems requires the homeless person to sign a consent form to share the data, which includes the answer to some intrusive questions about their sexual history or drug dealings. Also, many other agencies can easily get the access to these data. In the widest sense, some of our personal data collected by some organizations can be accessible to the state agencies, such as the police. It helps form digital surveillance, and police can use these data to exert the limitation on housing, traffic, travel even human rights of suspicious people. Therefore, the track of personal data provides a way to control the criminal suspects except for the arrest or jails. Such limitation can also be understood as electronic incarceration.

## *What do you understand this to mean? "systems act as a kind of 'empathy-overwrite'"*

Because of the limited houses, the system can allocate only a few tramps a house. Therefore, the system tends to use algorithm to decide whether a unhoused person deserves a house or not. And the source is the score of a survey that is the same for everyone. Then the algorithm ranks the unhoused from the most vulnerable to the least vulnerable. This process is more like a moral diagnosis rather than an universal evaluation. The higher the score is, the more probable to get a house. This method oversimplifies the problems, since it neglects the individual difference and issues. Indeed, all unhoused people need help and it is complex to evaluate who deserves a house. Only using survey based on moral standard is an act of 'empathy-overwrite'.  

## *China is much more advanced and expansive when it comes to applying technical solutions to societal processes or instant challenges (recent example). Try to point example cases in China that are in accordance or in opposition to the problematics discussed in the podcast. Perhaps you can think of"technical systems not well thought-through about what their impact on human beings is"*

It reminds me of the application of facial recognition in China. Nowadays, people use facial recognition technology for the payment, the check-in and many other daily activities. For example, every time I go to the airport or train station, I am required to take a photo to pass the identity verification. However, recently, people are concerned about this technology. Firstly, they are worried about the safety of their personal data. Sometimes people can only use a photo to pass the verification and then commit theft or other crimes. Also, the government has the access to these data(facial information), which means a new way of surveillance. The government can use these data to track rule-breakers or the suspect. Finally, in some cases, this technology leaves no option for the users. For example, earlier December in 2019, Guo Bing, a university professor, announced he was suing Hangzhou Safari Park for enforcing facial recognition. Prof Guo is a season ticket holder at the park, and he had used his fingerprint to enter for years, but was no longer able to do so because of the application of facial recognition. 
